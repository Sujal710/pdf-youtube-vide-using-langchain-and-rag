# -*- coding: utf-8 -*-
"""rag-based-qa.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Dj_NuTGY5WFZ3XtzvYgnbjp25g1CCb0R
"""

!pip install pandas sentence-transformers transformers scikit-learn

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import fitz  # PyMuPDF
# import json
# import numpy as np
# import requests
# from bs4 import BeautifulSoup
# from sentence_transformers import SentenceTransformer
# from sklearn.metrics.pairwise import cosine_similarity
# from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM
# 
# # === Load Embedding Model ===
# embedder = SentenceTransformer("all-MiniLM-L6-v2")
# 
# # QG - valhalla t5-small
# qg_tokenizer = AutoTokenizer.from_pretrained("valhalla/t5-small-qg-hl")
# qg_model = AutoModelForSeq2SeqLM.from_pretrained("valhalla/t5-small-qg-hl").cpu()
# qg = pipeline("text2text-generation", model=qg_model, tokenizer=qg_tokenizer, device=-1)
# 
# # LLM Answering - Google FLAN-T5-Small
# llm_tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-small")
# llm_model = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-small").cpu()
# llm = pipeline("text2text-generation", model=llm_model, tokenizer=llm_tokenizer, device=-1)
# 
# # === Utils ===
# def extract_text_from_pdf(uploaded_file):
#     doc = fitz.open(stream=uploaded_file.read(), filetype="pdf")
#     return "\n".join(page.get_text() for page in doc)
# 
# def extract_text_from_web(url):
#     try:
#         r = requests.get(url, timeout=10)
#         soup = BeautifulSoup(r.text, "html.parser")
#         return soup.get_text(separator="\n", strip=True)[:3000]
#     except:
#         return ""
# 
# def chunk_text(text, max_words=80):
#     lines = [line.strip() for line in text.split("\n") if line.strip()]
#     chunks, current = [], ""
#     for line in lines:
#         if len((current + " " + line).split()) <= max_words:
#             current += " " + line
#         else:
#             chunks.append(current.strip())
#             current = line
#     if current:
#         chunks.append(current.strip())
#     return chunks
# 
# def generate_qa(chunks):
#     pairs = []
#     for ch in chunks:
#         try:
#             question = qg(f"generate question: {ch}", max_length=64, do_sample=False)[0]['generated_text']
#             pairs.append({"question": question.strip(), "answer": ch.strip()})
#         except:
#             continue
#     return pairs
# 
# def embed_qa(pairs):
#     for item in pairs:
#         combo = f"Q: {item['question']}\nA: {item['answer']}"
#         item["embedding"] = embedder.encode(combo).tolist()
#     return pairs
# 
# def generate_precise_answer(query, context, max_tokens=100):
#     prompt = f"Answer the question based only on the context.\nContext: {context}\nQuestion: {query}"
#     try:
#         response = llm(prompt, max_new_tokens=max_tokens, do_sample=False)[0]["generated_text"]
#         return response.strip().split("\n")[0]
#     except:
#         return "Sorry, couldn't generate an answer."
# 
# def retrieve_answer(query, qa_data, precise=True):
#     embeddings = np.array([item["embedding"] for item in qa_data])
#     answers = [item["answer"] for item in qa_data]
#     q_emb = embedder.encode(query).reshape(1, -1)
#     sims = cosine_similarity(q_emb, embeddings)[0]
#     idx = np.argmax(sims)
#     if precise:
#         return generate_precise_answer(query, answers[idx])
#     else:
#         return answers[idx]
# 
# # === Streamlit UI ===
# st.set_page_config(page_title="FLAN-T5 RAG QA", layout="wide")
# st.title("ðŸ“˜ RAG QA App â€“ PDF / Web / JSON using FLAN-T5")
# 
# if "qa_data" not in st.session_state:
#     st.session_state.qa_data = []
# 
# mode = st.selectbox("Choose Input Source", ["Upload PDF", "Enter Website URL", "Upload Q&A JSON"])
# st.session_state.mode = mode  # Track which source was used
# 
# if mode == "Upload PDF":
#     uploaded_pdf = st.file_uploader("Upload PDF", type=["pdf"])
#     if uploaded_pdf and st.button("Process PDF"):
#         text = extract_text_from_pdf(uploaded_pdf)
#         chunks = chunk_text(text)
#         qa_pairs = generate_qa(chunks)
#         st.session_state.qa_data = embed_qa(qa_pairs)
#         st.success(f"âœ… Extracted {len(st.session_state.qa_data)} Q&A pairs.")
# 
# elif mode == "Enter Website URL":
#     url = st.text_input("Enter webpage URL")
#     if url and st.button("Process Website"):
#         text = extract_text_from_web(url)
#         chunks = chunk_text(text)
#         qa_pairs = generate_qa(chunks)
#         st.session_state.qa_data = embed_qa(qa_pairs)
#         st.success(f"âœ… Extracted {len(st.session_state.qa_data)} Q&A pairs.")
# 
# elif mode == "Upload Q&A JSON":
#     uploaded_json = st.file_uploader("Upload Q&A JSON", type=["json"])
#     if uploaded_json:
#         raw = uploaded_json.read().decode("utf-8")
#         try:
#             data = json.loads(raw)
#             if "embedding" not in data[0]:
#                 st.session_state.qa_data = embed_qa(data)
#             else:
#                 st.session_state.qa_data = data
#             st.success(f"âœ… Loaded {len(st.session_state.qa_data)} pairs.")
#         except Exception as e:
#             st.error(f"âŒ Failed to parse JSON: {e}")
# 
# # === Question Answering Section ===
# if st.session_state.qa_data:
#     query = st.text_input("Ask a question:")
#     if query:
#         is_precise = st.session_state.mode == "Upload Q&A JSON"
#         answer = retrieve_answer(query, st.session_state.qa_data, precise=is_precise)
#         st.markdown(f"### ðŸ’¡ Answer:\n{answer}")
#

!pip install  streamlit

!pip install PyMuPDF

!pip install pyngrok

import subprocess
import time
import requests
from pyngrok import ngrok, conf
import os

# Kill stray processes
os.system("pkill -f streamlit")
os.system("pkill -f ngrok")

# Set auth token (one-time)
ngrok.set_auth_token("2z2o0ehsGOvrLu13LgNhMaoqMth_5ynsuss3Z2QdcuAFnCAfA")

# Start Streamlit
print("ðŸš€ Starting Streamlit...")
process = subprocess.Popen(["streamlit", "run", "app.py"], stdout=subprocess.PIPE, stderr=subprocess.STDOUT, universal_newlines=True)

# Wait for Streamlit
def wait_until_ready(timeout=60):
    for _ in range(timeout):
        try:
            res = requests.get("http://localhost:8501")
            if res.status_code == 200:
                return True
        except requests.exceptions.ConnectionError:
            pass
        time.sleep(1)
    return False

# Open tunnel
if wait_until_ready():
    try:
        print("ðŸ”— Creating ngrok tunnel...")
        public_url = ngrok.connect(8501, bind_tls=True)
        print(f"âœ… Public URL: {public_url}")
    except Exception as e:
        print(f"âŒ ngrok error: {e}")
else:
    print("âŒ Streamlit not ready. Please check for errors in app.py.")

# Keep alive
try:
    while True:
        output = process.stdout.readline()
        if output:
            print(output.strip())
        time.sleep(0.1)
except KeyboardInterrupt:
    print("ðŸ›‘ Cleaning up...")
    ngrok.disconnect(public_url)
    ngrok.kill()
    process.terminate()

!pkill -f streamlit
!pkill -f ngrok

from pymongo import MongoClient

!pip install pymongo

from sentence_transformers import SentenceTransformer
import fitz
import numpy as np

client=MongoClient('localhost:27017')

client

db=client['pdfs']
table=db['first']

embedder=SentenceTransformer('all-MiniLM-L6-v2')

from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import MongoDBAtlasVectorSearch
from langchain.chains import RetrievalQA
from langchain.llms import HuggingFaceHub
from pymongo import MongoClient


# -------------------------------
# 1. PDF Loader
# -------------------------------
pdf_path = "/content/Ramayana-VOL-2-Aranya-Kishkindha-Sundara-Kanda.pdf"
loader = PyPDFLoader(pdf_path)
pages = loader.load()

# -------------------------------
# 2. Split into Chunks
# -------------------------------
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
docs = splitter.split_documents(pages)

# -------------------------------
# 3. HuggingFace Embedding Model
# -------------------------------
embedder = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# -------------------------------
# 4. MongoDB Vector Store
# -------------------------------
client = MongoClient("localhost:27017")
db = client["pdfs"]
collection = db["first"]

vectorstore = MongoDBAtlasVectorSearch(
    collection=collection,
    embedding=embedder,
    index_name="vector_index"
)

# Store chunks + embeddings in MongoDB
vectorstore.add_documents(docs)

# -------------------------------
# 5. HuggingFace LLM for QA
# -------------------------------
llm = HuggingFaceHub(
    repo_id="google/flan-t5-base",  # You can replace with other open-source LLMs
    model_kwargs={"temperature": 0.2, "max_length": 256},
    huggingfacehub_api_token="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"
)

# -------------------------------
# 6. Retrieval + QA Chain
# -------------------------------
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 3}),
    return_source_documents=True
)

# -------------------------------
# 7. Ask a Question
# -------------------------------
query = "Who is ram?"
result = qa_chain({"query": query})

# -------------------------------
# 8. Show Result
# -------------------------------
print("\nðŸ§  Answer:\n", result['result'])
print("\nðŸ“š Source Documents:\n", result['source_documents'])

!pip install langchain pymongo sentence-transformers transformers huggingface_hub

!pip install langchain-community

!pip install pypdf

!docker run -d -p 27017:27017 --name mongodb_container mongo

from pymongo import MongoClient
import urllib.parse

# MongoDB Atlas credentials
username = urllib.parse.quote_plus("sujalthakkar56")
password = urllib.parse.quote_plus("sujal")  # Replace with actual password

# Correct URI with TLS options
uri = f"mongodb+srv://{username}:{password}@cluster0.4hudbga.mongodb.net/?retryWrites=true&w=majority&tls=true&tlsAllowInvalidCertificates=true"

client = MongoClient(uri)

# Test connection
print(client.list_database_names())

from pymongo import MongoClient

# Use the full connection string with tlsAllowInvalidCertificates
uri = "mongodb+srv://sujalthakkar56:zhbrXQqjXKI8QLaH@cluster0.l1cdexi.mongodb.net/?retryWrites=true&w=majority&tlsAllowInvalidCertificates=true"

# Connect to the client
client = MongoClient(uri)

# Check existing databases
print(client.list_database_names())

mongodb+srv://sujalthakkar56:zhbrXQqjXKI8QLaH@cluster0.l1cdexi.mongodb.net/

!pip install pymongo[srv]==4.6.3 dnspython

from langchain_community.document_loaders import PyPDFium2Loader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import MongoDBAtlasVectorSearch
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from pymongo import MongoClient, WriteConcern
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig
from langchain_huggingface import HuggingFacePipeline
import torch
import os
import datetime

# -------------------------------
# 1. Configuration
# -------------------------------
MONGODB_URI = "mongodb+srv://sujalthakkar56:zhbrXQqjXKI8QLaH@cluster0.l1cdexi.mongodb.net/"

# -------------------------------
# 2. Load PDF
# -------------------------------
print("ðŸ“„ Loading PDF...")
loader = PyPDFium2Loader(r"/content/Building Machine Learning Systems with Python - Second Edition.pdf")
pages = loader.load()
print(f"âœ… Loaded {len(pages)} pages")

# -------------------------------
# 3. Split text into chunks
# -------------------------------
splitter = RecursiveCharacterTextSplitter(
    chunk_size=800,
    chunk_overlap=150,
    length_function=len,
    add_start_index=True
)
docs = splitter.split_documents(pages)
print(f"âœ… Total Chunks: {len(docs)}")

for doc in docs:
    doc.metadata["processed_at"] = datetime.datetime.utcnow().isoformat()

# -------------------------------
# 4. Connect to MongoDB
# -------------------------------
print("ðŸŒ Connecting to MongoDB Atlas...")
client = MongoClient(MONGODB_URI)
client.admin.command("ping")
db = client["book"]
collection = db["one"].with_options(write_concern=WriteConcern(w="majority"))
deleted = collection.delete_many({})
print(f"ðŸ§¹ Cleared old documents: {deleted.deleted_count}")

# -------------------------------
# 5. Embeddings & Vector Store
# -------------------------------
print("ðŸ§  Generating embeddings...")
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-mpnet-base-v2",
    model_kwargs={"device": "cuda" if torch.cuda.is_available() else "cpu"},
    encode_kwargs={"normalize_embeddings": True}
)

print("ðŸ“¦ Creating vector store in MongoDB...")
vector_store = MongoDBAtlasVectorSearch.from_documents(
    documents=docs,
    embedding=embeddings,
    collection=collection,
    index_name="vector_index"
)
stored = collection.count_documents({})
print(f"âœ… Stored in MongoDB: {stored} chunks")

# -------------------------------
# 6. Load Quantized LLM (Optimized for Colab)
# -------------------------------
print("ðŸ¤– Loading optimized LLM for Colab...")

# 4-bit quantization config
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True
)

model_name = "mistralai/Mistral-7B-Instruct-v0.1"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto"
)

pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=300,
    temperature=0.3,
    top_p=0.9,
    device_map="auto"
)

llm = HuggingFacePipeline(pipeline=pipe)

# -------------------------------
# 7. Create QA Chain
# -------------------------------
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vector_store.as_retriever(
        search_type="mmr",
        search_kwargs={"k": 6, "fetch_k": 25, "lambda_mult": 0.6}
    ),
    return_source_documents=True
)
print("âœ… QA system is ready!")

# -------------------------------
# 8. Ask Questions
# -------------------------------
questions = [
    "How is the Iris dataset used to explain classification?",
    "What is the approach taken to cluster related posts using text data?",
    "What is the book's explanation of topic modeling with Latent Dirichlet Allocation (LDA)?",
    "How is logistic regression applied to detect poor-quality answers?"
]

print("\n" + "="*50)
print("ðŸ“š Starting QA Session".center(50))
print("="*50)

for question in questions:
    print(f"\nâ“ Question: {question}")
    try:
        result = qa_chain.invoke({"query": question})
        print(f"\nðŸ’¡ Answer:\n{result['result']}")

        print("\nðŸ” Top Sources:")
        for i, doc in enumerate(result["source_documents"][:2]):
            print(f"\nðŸ“‘ Source {i+1}: Page {doc.metadata.get('page', 'N/A')}")
            print(doc.page_content[:300] + "...")
    except Exception as e:
        print(f"\nâš ï¸ Error processing question: {str(e)}")

print("\n" + "="*50)
print("âœ… QA Session Complete".center(50))
print("="*50)

import ssl
print(ssl.OPENSSL_VERSION)

import certifi
from pymongo import MongoClient
import urllib.parse

# Credentials
username = urllib.parse.quote_plus("sujalthakkar56")
password = urllib.parse.quote_plus("zhbrXQqjXKI8QLaH")

# URI
uri = f"mongodb+srv://{username}:{password}@cluster0.l1cdexi.mongodb.net/?retryWrites=true&w=majority"

# Secure client with certificate
client = MongoClient(
    uri,
    tls=True,
    tlsCAFile=certifi.where(),  # âœ… Use trusted CA
    serverSelectionTimeoutMS=5000
)

# Ping MongoDB
try:
    client.admin.command("ping")
    print("âœ… MongoDB Atlas connection successful")
except Exception as e:
    print("âŒ Connection failed:", e)

!pip install certifi

!pip install --upgrade certifi

!pip install pymongo

!pip install pytesseract

"""This command will:
- `docker run`: Run a Docker container.
- `-d`: Run the container in detached mode (in the background).
- `-p 27017:27017`: Map the host's port 27017 to the container's port 27017, allowing your notebook to connect to the database.
- `--name mongodb_container`: Give the container a name for easy identification.
- `mongo`: Specify the official MongoDB Docker image to use.

After running this cell, the MongoDB server should be running, and you can try executing the previous cell (`rPiVXojLfmCC`) again to load and process the PDF.
"""

!pip install pypdfium2

import streamlit as st
from langchain_community.document_loaders import PyPDFium2Loader
from lanchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import MongoDBAtlasVectorSearch
from langchain_huggingface import HuggingFaceEmbedding,HuggingFacePipeline
from langchain.chains import retrieval_qa
from pymango import MangoClient
import datetime
import torch
import os

MONGODB_URI= "mongodb+srv://sujalthakkar56:zhbrXQqjXKI8QLaH@cluster0.l1cdexi.mongodb.net/"
MODEL_NAME="google/flan-t5-small"
st.set_page_config(page_title="PDF QA BOT",layout="wide")
st.title("ðŸ“„ PDF QA Chatbot (MongoDB + HuggingFace)")
upload_file=st.file_uploader("upload your file",type=["pdf"])
questin_input=st.text_area('enter your question')
if st.button('run qa') and upload_file and question_input.strip():
  questions=[q.strip for q in questions_input.strip().split('\n') if q.strip()]
  with st.spinner("ðŸ“„ Processing PDF..."):
    with open("temp.pdf","wb") as f:
      f.write(upload_file.read())
    loader=Pypdfium2Loader("temp.pdf")
    pages=loader.load()
    splitter=RecursiveCharacterTextSplitter(chunk_size=600,chunk_overlap=100)
    docs=splitter.split_documents(pages)
    for doc in docs:
      doc.metadata["processed_at"]=datetime.datetime.utcnow().isoformat()
    with st.spinner('connecting to mongo db'):
      client=MongoClient(MONGODB_URI)
      collection=client["book"]["one"]
      collection.delete_many({})
    with st.spinner('generating embeddings'):
      embeddings=HuggingFaceEmbeddings(
          model_name="sentence-transformers/all-mpnet-base-v2",
          model_kwargs={"device":"cuda" if torch.cuda.is_available() else "cpu"}
      )
      vector_store=MongoDBAtlasVectorSearch.from_documents(documents=docs,embedding=embeddings,collection=collection,index_name=vector_index)
      with st.spinner('generating embedding'):
        tokenizer=AutoTokenizer.from_pretrained(model_name)
        m_name=AutoModelForSeq2SeqLM.from_pretrained(model_name)
        pipe=pipeline( "text2text-generation",
            model=model,
            tokenizer=tokenizer,
            max_new_tokens=200,
            temperature=0.4,
            device="cuda" if torch.cuda.is_available() else "cpu")
        llm=HuggingFacePipeline(pipeline=pipe)
        retriever=vector_store.as_retriever(search_type="similarity",search_kwargs={"k":4})
        qa_chain=RetrievalQA.from_chain_type(llm=llm,chain_type="stuff",retriever=retriever,return_source_documents=True)
        st.success("âœ… System Ready. See Answers Below:")
        for i,q in enumerate(questions):
          with st.spinner(f"ðŸ” Answering Q{i+1}: {q}"):
            result=qa_chain.invoke({"query":q})
            st.markdown(f"### â“ {q}")
            st.write(f"ðŸ’¡ **Answer:** {result['result']}")
            st.markdown("ðŸ“š **Sources (Simulated Relevance):**")
            for j,doc in enumerate(result['source_document'][:2]):
              page=doc.metadata.get('page','N/A')
              simulated_score=round(1 - (j * 0.1),2)
              st.markdown(f"**Source {j+1} â€” Page {page} | similarty_score: {simulated_score}**")
              st.code(doc.page_content[:300] + "...",language="markdown")

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# from langchain_community.document_loaders import PyPDFium2Loader
# from langchain.text_splitter import RecursiveCharacterTextSplitter
# from langchain.vectorstores import MongoDBAtlasVectorSearch
# from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline
# from langchain.chains import RetrievalQA
# from pymongo import MongoClient
# from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline
# import datetime
# import torch
# 
# # -------------------------------
# # MongoDB & Model Configuration
# # -------------------------------
# MONGODB_URI = "mongodb+srv://sujalthakkar56:zhbrXQqjXKI8QLaH@cluster0.l1cdexi.mongodb.net/"
# MODEL_NAME = "google/flan-t5-small"
# DB_NAME = "book"
# COLLECTION_NAME = "one"
# 
# SIMILARITY_INDEXES = {
#     "Cosine": "vector_index_cosine",
#     "Dot Product": "vector_index_dot",
#     "Euclidean": "vector_index_euclidean"
# }
# 
# # -------------------------------
# # Streamlit UI
# # -------------------------------
# st.set_page_config(page_title="PDF QA Bot", layout="wide")
# st.title("ðŸ“˜ PDF QA Chatbot with MongoDB Vector Search")
# 
# uploaded_file = st.file_uploader("ðŸ“Ž Upload your PDF file", type=["pdf"])
# questions_input = st.text_area("ðŸ’¬ Enter your questions (one per line)")
# similarity_choice = st.selectbox("ðŸ“ Choose Similarity Metric", list(SIMILARITY_INDEXES.keys()))
# 
# if st.button("Run QA") and uploaded_file and questions_input.strip():
#     questions = [q.strip() for q in questions_input.strip().split("\n") if q.strip()]
# 
#     with st.spinner("ðŸ“„ Processing PDF..."):
#         with open("temp.pdf", "wb") as f:
#             f.write(uploaded_file.read())
#         loader = PyPDFium2Loader("temp.pdf")
#         pages = loader.load()
# 
#         splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=100)
#         docs = splitter.split_documents(pages)
#         for doc in docs:
#             doc.metadata["processed_at"] = datetime.datetime.utcnow().isoformat()
# 
#     with st.spinner("ðŸ”— Connecting to MongoDB Atlas..."):
#         client = MongoClient(MONGODB_URI)
#         db = client[DB_NAME]
#         collection = db[COLLECTION_NAME]
#         collection.delete_many({})
# 
#     with st.spinner("ðŸ§  Generating embeddings..."):
#         embeddings = HuggingFaceEmbeddings(
#             model_name="sentence-transformers/all-mpnet-base-v2",
#             model_kwargs={"device": "cuda" if torch.cuda.is_available() else "cpu"}
#         )
# 
#         MongoDBAtlasVectorSearch.from_documents(
#             documents=docs,
#             embedding=embeddings,
#             collection=collection,
#             index_name=SIMILARITY_INDEXES[similarity_choice]
#         )
# 
#     with st.spinner("ðŸ¤– Loading FLAN-T5 model..."):
#         tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
#         model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)
#         pipe = pipeline(
#             "text2text-generation",
#             model=model,
#             tokenizer=tokenizer,
#             max_new_tokens=200,
#             temperature=0.4,
#             device="cuda" if torch.cuda.is_available() else "cpu"
#         )
#         llm = HuggingFacePipeline(pipeline=pipe)
# 
#     st.success(f"âœ… Model Ready Using {similarity_choice} Similarity")
# 
#     # Manual retriever to access scores
#     vector_search = MongoDBAtlasVectorSearch(
#         collection=collection,
#         embedding=embeddings,
#         index_name=SIMILARITY_INDEXES[similarity_choice]
#     )
#     retriever = vector_search.as_retriever(
#         search_type="similarity",
#         search_kwargs={"k": 4}
#     )
# 
#     qa_chain = RetrievalQA.from_chain_type(
#         llm=llm,
#         chain_type="stuff",
#         retriever=retriever,
#         return_source_documents=True
#     )
# 
#     for i, q in enumerate(questions):
#         with st.spinner(f"ðŸ’¬ Answering Q{i+1}: {q}"):
#             # Get docs + scores manually
#             docs_with_scores = vector_search.similarity_search_with_score(q, k=4)
#             top_docs = [doc for doc, _ in docs_with_scores]
# 
#             # Answer using top documents
#             answer = qa_chain.combine_documents_chain.run(input_documents=top_docs, question=q)
# 
#             st.markdown(f"### â“ {q}")
#             st.write(f"ðŸ’¡ **Answer:** {answer}")
#             st.markdown("ðŸ“š **Top Sources with Similarity Scores:**")
#             for j, (doc, score) in enumerate(docs_with_scores[:2]):
#                 page = doc.metadata.get("page", "N/A")
#                 st.markdown(f"**Source {j+1} â€” Page {page} | Score: {round(score, 4)}**")
#                 st.code(doc.page_content[:300] + "...", language="markdown")
# 
#     st.success("ðŸŽ¯ QA Complete")
#

!pip install langchain_community

!pip install langchain_huggingface

!pip install pymongo

!pip install pyngrok

import subprocess
import time
import requests
from pyngrok import ngrok

# âœ… Set ngrok authtoken (only once per machine)
ngrok.set_auth_token("2z2o0ehsGOvrLu13LgNhMaoqMth_5ynsuss3Z2QdcuAFnCAfA")

# âœ… Start Streamlit in background
print("ðŸš€ Starting Streamlit...")
process = subprocess.Popen(["streamlit", "run", "app.py"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)

# ðŸ” Wait until Streamlit server is ready
def wait_until_streamlit_ready(timeout=30):
    for _ in range(timeout):
        try:
            res = requests.get("http://localhost:8501")
            if res.status_code == 200:
                return True
        except requests.exceptions.ConnectionError:
            time.sleep(1)
    return False

if wait_until_streamlit_ready():
    # âœ… Start ngrok tunnel
    public_url = ngrok.connect(8501)
    print(f"ðŸ”— Public URL: {public_url}")
    print("ðŸ“¡ Tunnel is active. Press Ctrl+C to stop.")
else:
    print("âŒ Streamlit did not start within the timeout period.")

# ðŸ” Keep alive
try:
    while True:
        time.sleep(1)
except KeyboardInterrupt:
    print("ðŸ›‘ Stopping ngrok...")
    ngrok.disconnect(public_url)
    ngrok.kill()
    process.terminate()

!pip install streamlit

!pip install pyngrok

